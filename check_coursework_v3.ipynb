{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIaqPNWwYn-c"
      },
      "source": [
        "#Neural network coursework checker\n",
        "This notebook is for checking ME4/MSc Machine Learning neural network coursework submission. You should run your model through this to check that you are getting the right answer prior to submission.\n",
        "**If you get the wrong answers out, this is an indication that you need to change your model, not an indication that this script is incorrect.**\n",
        "\n",
        "Also note that while this script may find many errors, it will not find all and that you are ultimately responsible for double-checking that what you submit is correct.\n",
        "\n",
        "**Note that this script is not used for assessment (but note that similar routines are used to load the model). The data used is just to confirm that the model has been loaded properly.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBmTOrSGY6UL"
      },
      "source": [
        "---\n",
        "###Tidy up space (not normally needed)\n",
        "The section below is just used for clearing any previously uploaded files. You should not normally need to run it, unless you have had crashes. If you run it you will need to re-upload the datasets. You can always open the file manager on the left hand side to look at the files too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73fIVRGcU490"
      },
      "outputs": [],
      "source": [
        "#clear all files from the work area - only really need to do this if uploading\n",
        "#files multiple times\n",
        "!ls -lh\n",
        "!rm *.csv\n",
        "!rm *.h5\n",
        "!rm *.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR1sP882ZYZf"
      },
      "source": [
        "---\n",
        "##Upload model and test with data\n",
        "This section loads the files selected (note -- select both .h5 and .txt for scaling parameters if you have them, otherwise it will not do any scaling), and runs the training data you have uploaded through the model. You should check that this all works OK -- you don't get any errors -- and that the fraction correct matches what you saw when you trained it. Note that this only works with one model at a time so you will need to run it twice -- once each for dataset 1 and 2.\n",
        "\n",
        "Also note that if you upload files with the same name multiple times then they will be renamed to e.g. xyz123-2 (1).h5 which will cause issues - the script tries to catch this by deleting any uploaded files at the end, but if it crashes you may need to delete all uploaded files using the section at the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AK8IyjA0PsgJ",
        "outputId": "2d981b4c-2a04-4005-a6a2-a51652c8969d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset considered is:  1\n",
            "ae1220-1.txt exists - loading in scaling parameters\n",
            "63/63 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Fraction correct with training data is: 0.9225**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#upload model files\n",
        "import numpy as np\n",
        "import os.path\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#import for bold font\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "#use this code in a non-Colab/Jupyter environment\n",
        "# #user defined parameters (change these as necessary):\n",
        "# #put your name here:\n",
        "username = 'ae1220'\n",
        "# #set which dataset to use:\n",
        "dataset = 1\n",
        "\n",
        "filebase = username.lower()+'-'+str(dataset)\n",
        "scale_exists = True\n",
        "h5file = 'ae1220-1.h5'\n",
        "scalefile = 'ae1220-1.txt'\n",
        "dataset = filebase[-1]\n",
        "print('Dataset considered is: ',dataset)\n",
        "\n",
        "#load in student model\n",
        "model = load_model(h5file,compile=False)\n",
        "\n",
        "if scale_exists:\n",
        "  if os.path.exists(scalefile):\n",
        "      print(scalefile+' exists - loading in scaling parameters')\n",
        "      scaleArray = np.loadtxt(scalefile)\n",
        "  else:\n",
        "      print(scalefile+' not found - assuming no scaling')\n",
        "      scaleArray = np.array([np.zeros([6,]), np.ones([6,])])\n",
        "else:\n",
        "    print('No scale file provided - assuming no scaling')\n",
        "    scaleArray = np.array([np.zeros([6,]), np.ones([6,])])\n",
        "\n",
        "#load in the data provided to the students\n",
        "df = pd.read_csv('http://pogo.software/me4ml/dataset' + str(dataset) + '.csv')\n",
        "\n",
        "Lt = np.array(df['Arm length (m)'][:])\n",
        "Wt = np.array(df['Ball weight (kg)'][:])\n",
        "Rt = np.array(df['Ball radius (mm)'][:])\n",
        "Tt = np.array(df['Air temperature (deg C)'][:])\n",
        "Et = np.array(df['Spring constant (N per m)'][:])\n",
        "Dt = np.array(df['Device weight (kg)'][:])\n",
        "Ot = np.array(df['Target hit'][:])\n",
        "XtUnscaled = np.column_stack([Lt, Wt, Rt, Tt, Et, Dt])\n",
        "\n",
        "# use values to scale validation data in XvUnscaled (whose shape is [number_of_validations,6])\n",
        "Xt = (XtUnscaled-scaleArray[0,:])/scaleArray[1,:]\n",
        "\n",
        "Yt = to_categorical(Ot)\n",
        "#run the data through the model\n",
        "Yt_predict = model.predict(Xt)\n",
        "\n",
        "#output a summary of the model if you wish\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "number_correct = 0\n",
        "for i in range(len(Yt)):\n",
        "    if np.round(Yt[i, 0]) == np.round(Yt_predict[i, 0]):\n",
        "        number_correct += 1\n",
        "\n",
        "fraction_correct = 1.0 * number_correct / len(Yt_predict)\n",
        "\n",
        "\n",
        "\n",
        "printmd(\"**Fraction correct with training data is: \"+str(fraction_correct)+\"**\")\n",
        "\n",
        "if fraction_correct < 0.6:\n",
        "    printmd('**Warning: very poor performance on training data; likely error**')\n",
        "\n",
        "#print('Tidying up - removing h5 file and scale file if it exists')\n",
        "os.remove(h5file)\n",
        "if scale_exists:\n",
        "  os.remove(scalefile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
